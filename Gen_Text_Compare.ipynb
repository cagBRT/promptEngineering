{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/uQrUt+fLprwJIsjWmm3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Gen_Text_Compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook the user gives three different language models the same text sequence and asks for the models to complete the text. <br>\n",
        "\n",
        "A comparison of the completed text from the models is interesting and can lead to a better understanding of each model. <br>\n",
        "\n",
        "The assignment asks the student to change the text for padding, then rerun the models and see the change in the completed text."
      ],
      "metadata": {
        "id": "Yse4iXLv0HmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/pytorch-transformers.git"
      ],
      "metadata": {
        "id": "UZCEIywKk6Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "zcReapPljYLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers"
      ],
      "metadata": {
        "id": "lxOTMChgjq5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "rfIDhZJI_Xok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "igFbpcOEHRgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead"
      ],
      "metadata": {
        "id": "HoGiRb9jkFwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "q19qUpIQuhL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finish the text:<br>\n",
        "\"What is the fastest car in the_________\""
      ],
      "metadata": {
        "id": "dPwB0j8ZkgbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GPT2 to finish the text"
      ],
      "metadata": {
        "id": "MS0SQrOEksLP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuoF_zVfjKnT"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "pad_token_id=tokenizer.eos_token_id\n",
        "attention_mask=None,\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "#tokens_tensor = tokens_tensor.to('cuda')\n",
        "#model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "# Print the predicted word\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "id": "2Ev5tESYkLJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b8DjeQsa4QYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2<br>\n",
        "Generative Pre-trained Transformer 2 (GPT-2) is an open-source artificial intelligence large language model created by OpenAI in February 2019<br>\n",
        "\n",
        "The GPT-2 model has **1.5 billion parameters**, and was trained on a dataset of 8 million web pages.<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "AE0YOz-F1B94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was **pretrained on the raw texts only, with no humans labelling them in any way** (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.<br>\n",
        "\n",
        "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right."
      ],
      "metadata": {
        "id": "ZyMll9JY12oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "URFcG6Yd2J5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-XL"
      ],
      "metadata": {
        "id": "e6ItqCLOCaub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "FlLZsG-xDwIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This pipelinefor transformer-XL model prodcues an error\n",
        "#Waiting for HuggingFace to send correction\n",
        "#tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\n",
        "#model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
        "#transfo_xl = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "#transfo_xl(\"Hello, I'm a language model,\", max_length=300)"
      ],
      "metadata": {
        "id": "YQWvyZsy7AUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install xformers"
      ],
      "metadata": {
        "id": "kbjxwnTCtDb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLNet<br>\n",
        "\n",
        "XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order.<br>\n",
        "\n",
        "Due to the difficulty of training a fully auto-regressive model over various factorization order, XLNet is pretrained using only a sub-set of the output tokens as target which are selected with the target_mapping input.<br>\n",
        "\n",
        "XLNet is one of the few models that has no sequence length limit."
      ],
      "metadata": {
        "id": "5HLje1xY2zUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xlnet_generator = pipeline(\"text-generation\", model=\"xlnet-base-cased\", tokenizer=\"xlnet-base-cased\")\n",
        "print(xlnet_generator(\"Hello, I'm a language model,\"))"
      ],
      "metadata": {
        "id": "1rqNVm714dNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating AutoCode using Salesforce Code Generator Model<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "fLEWs7ptvq4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator = pipeline(\"text-generation\",model=\"Salesforce/codegen-350M-mono\")\n",
        "text= text_generator(\"Hello, I'm a language model, \",  max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "TgGVMWq_GT7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample output from Salesforce/codegen-350M-mono**<br>\n",
        "\n",
        "[{'generated_text': \"Hello, I'm a language model, \\n        I use simple neural \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0network to learn\\u200b\\u200b\\tto map\\t\\u200b\"}, {'generated_text': \"Hello, I'm a language model, \\n# and I am a student whose work is developing a language model. \\n#\\n# The\"}, {'generated_text': \"Hello, I'm a language model, \\n# which is a language model that uses numpy to transform a text into its word representation. \"}, {'generated_text': \"Hello, I'm a language model, \\n# so you can learn about the structure of an entity or sentence - we're trying to get as\"}, {'generated_text': 'Hello, I\\'m a language model, \\n    I\\'m the only author here!\\n    \\'\\'\\'\\n    \\n    \"\"\"\\n    @'}]"
      ],
      "metadata": {
        "id": "2hNsXCXEt9Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "5r5-PISrGtKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an autoregressive language model, CodeGen is capable of extracting features from given natural language and programming language texts, and calculating the likelihood of them. <br>\n",
        "\n",
        "**However, the model is intended for and best at program synthesis, that is, generating executable code given English prompts, where the prompts should be in the form of a comment string. The model can complete partially-generated code as well.**"
      ],
      "metadata": {
        "id": "9rTjkurgu2zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/Salesforce/codegen-350M-mono?text=This+function+adds+two+floats+together+and+returns+the+sum\n",
        "\n"
      ],
      "metadata": {
        "id": "Bizt3dAGwN9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "\n",
        "text = \"This function adds two floats together and returns the sum\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "generated_ids = model.generate(input_ids, max_length=128)\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "R56soEacuLTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/xlnet-base-cased\n"
      ],
      "metadata": {
        "id": "xaxocrhExGTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8VG3NCLQCf8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we copy and paste the same text into three different models. <br>\n",
        "Compare the outputs and the length of time for each model"
      ],
      "metadata": {
        "id": "N_aPzOHbt0W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete the text:<br>\n",
        "**Copy and paste the following text into the model prompt**<br>\n",
        "The unicorns had seemed to know each other almost as well as they did common humans. The study was published in Science Translational Medicine on May 6. What's more, researchers found that five percent of the unicorns recognized each other well. The study team thinks this might translate into a future where humans would be able to communicate more clearly with those known as super Unicorns. And if we're going to move ahead with that future, we've got to do it at least a"
      ],
      "metadata": {
        "id": "rgotMIsklEZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R2XPXXAo4UKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-2**"
      ],
      "metadata": {
        "id": "C1uSgtt3Clmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py \\\n",
        "        --model_type=gpt2 --length=100 --model_name_or_path=gpt2"
      ],
      "metadata": {
        "id": "Tdt53Wtjk4pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample of Completed text from GPT2\n",
        "little faster than when unicorns first entered human speech.<br>\n",
        "\n",
        "This is nice news to hear.<br>\n",
        "\n",
        "What do you think about Devesque's research? Is it worth it? Tell us what you think in the comments below or tweet @TheCultula_.<|endoftext|>"
      ],
      "metadata": {
        "id": "03nKEcy-n_jX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5ULALoPOozRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample of Completed text from XLNet"
      ],
      "metadata": {
        "id": "Y246FAGknPP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py  \\\n",
        "    --model_type=xlnet \\\n",
        "    --length=100 \\\n",
        "    --model_name_or_path=xlnet-base-cased"
      ],
      "metadata": {
        "id": "I0oLwxASmzAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bit first.<eop> August 26, 2017In Singapore, the White Party spent the second day on their frontline road. The White Party, preferably the White House, spent the first day in a military parking garage in North Galleria, Washington, a two-day military \"preview session.\" The Democratic Party of Russia, representing the coalition with the White House, spent the first day in a military parking garage, a parking yard in East North Galleria, a parking garage in North\n"
      ],
      "metadata": {
        "id": "tG1umn7coXaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Gn_gS4RWsEXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample of Completed text from transformer-XL"
      ],
      "metadata": {
        "id": "fEDDkiQ2sFL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll see a message about model variables, for now we can ignore them. <br>\n",
        "**You'll have to be patient for the text completion - this can take a few minutes.**"
      ],
      "metadata": {
        "id": "t34BUidQuBkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py  \\\n",
        "    --model_type=transfo-xl \\\n",
        "    --length=100 \\\n",
        "    --model_name_or_path=transfo-xl-wt103"
      ],
      "metadata": {
        "id": "HDu7bE_Npa_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Completed by transformer-XL\n",
        "dozen times. For example, if you lose your head or your body, then you can't fight as a horse or lead a more manly life. There's always a chance of something changing. Today, we're going to do it every day, and that's exactly where you want to go. The knight is going to stop hunting, and he's going to allow a unicorn to come down. The knight will be so beautiful that he'll start stalking you until you've killed him. <eos>"
      ],
      "metadata": {
        "id": "VHGMxA57p3Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pmFfqMNH0Xj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Blooms model**<br>\n",
        "\n",
        "We recommend using the model to perform tasks expressed in natural language. <br><br>\n",
        "**For example,** given the prompt \"Translate to English: Je t’aime.\",<br>\n",
        " >the model will most likely answer \"I love you.\".\n",
        "\n",
        "<br>\n",
        "\n",
        "**Some prompt ideas from our paper:**\n",
        "\n",
        "一个传奇的开端，一个不灭的神话，这不仅仅是一部电影，而是作为一个走进新时代的标签，永远彪炳史册。你认为这句话的立场是赞扬、中立还是批评?\n",
        "Suggest at least five related search terms to \"Mạng neural nhân tạo\".<br>\n",
        "Write a fairy tale about a troll saving a princess from a dangerous dragon. The fairy tale is a masterpiece that has achieved praise worldwide and its moral is \"Heroes <br>Come in All Shapes and Sizes\". Story (in Spanish)\n",
        "\n",
        "Explain in a sentence in Telugu what is backpropagation in neural networks.\n",
        "\n",
        "https://huggingface.co/bigscience/bloomz-560m?text=Why+is+the+sky+blue%3F+blue+light\n"
      ],
      "metadata": {
        "id": "iZdS0sTL0Y3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UNRff3aarFaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**text2image prompt generator**\n",
        "\n",
        "This prompt generator can be used to auto-complete prompts for any text-to-image model (including the DALL·E family)\n",
        "<br><br>\n",
        "Note that, while this model can be used together with any text-to-image model, it occasionally produces Midjourney-specific tags. Users can specify certain requirements via double-dashed parameters (e.g. --ar 16:9 sets the aspect ratio to 16:9, and --no snake asks the model to exclude snakes from the generated image) or set the importance of various entities in the image via explicit weights (e.g. hot dog::1.5 food::-1 is likely to produce the image of an animal instead of a frankfurter).\n",
        "\n",
        "\n",
        "\n",
        "https://huggingface.co/succinctly/text2image-prompt-generator?text=Once+upon+a+time%2C"
      ],
      "metadata": {
        "id": "urmw1xGm2yOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2RmfMUIZ3lP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate prompts or titles of given text**\n",
        "\n",
        "Excellent accuracy for one line prompts. Prompts can be used for image generation, title or meta descriptions.\n",
        "\n",
        "\n",
        "https://huggingface.co/SamAct/PromptGeneration-base title, prompt generation\n"
      ],
      "metadata": {
        "id": "F0uDHjZZ3mFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**<br>\n",
        "Pick a model and try different inputs.\n"
      ],
      "metadata": {
        "id": "6L0ht1HsrG1Z"
      }
    }
  ]
}