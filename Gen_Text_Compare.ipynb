{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuvZbV7nBxxn7m//cMwxG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Gen_Text_Compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook the user gives three different language models the same text sequence and asks for the models to complete the text. <br>\n",
        "\n",
        "A comparison of the completed text from the models is interesting and can lead to a better understanding of each model. <br>\n",
        "\n",
        "The assignment asks the student to change the text for padding, then rerun the models and see the change in the completed text."
      ],
      "metadata": {
        "id": "Yse4iXLv0HmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/pytorch-transformers.git"
      ],
      "metadata": {
        "id": "UZCEIywKk6Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "zcReapPljYLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers"
      ],
      "metadata": {
        "id": "lxOTMChgjq5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "rfIDhZJI_Xok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "igFbpcOEHRgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead"
      ],
      "metadata": {
        "id": "HoGiRb9jkFwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "q19qUpIQuhL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finish the text:<br>\n",
        "\"What is the fastest car in the_________\""
      ],
      "metadata": {
        "id": "dPwB0j8ZkgbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GPT2 to finish the text"
      ],
      "metadata": {
        "id": "MS0SQrOEksLP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuoF_zVfjKnT"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "pad_token_id=tokenizer.eos_token_id\n",
        "attention_mask=None,\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "#tokens_tensor = tokens_tensor.to('cuda')\n",
        "#model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "# Print the predicted word\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "id": "2Ev5tESYkLJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b8DjeQsa4QYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2<br>\n",
        "Generative Pre-trained Transformer 2 (GPT-2) is an open-source artificial intelligence large language model created by OpenAI in February 2019<br>\n",
        "\n",
        "The GPT-2 model has **1.5 billion parameters**, and was trained on a dataset of 8 million web pages.<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "AE0YOz-F1B94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was **pretrained on the raw texts only, with no humans labelling them in any way** (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.<br>\n",
        "\n",
        "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right."
      ],
      "metadata": {
        "id": "ZyMll9JY12oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "URFcG6Yd2J5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-XL"
      ],
      "metadata": {
        "id": "e6ItqCLOCaub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "FlLZsG-xDwIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")\n",
        "model = TransfoXLModel.from_pretrained(\"transfo-xl-wt103\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "6MUyrI8_ED-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\n",
        "model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
        "transfo_xl = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "transfo_xl(\"Hello, I'm a language model,\", max_length=300)"
      ],
      "metadata": {
        "id": "YQWvyZsy7AUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator = pipeline(\"text-generation\",model=\"facebook/opt-6.7b\")\n",
        "text= text_generator(\"Hello, I'm a language model, \",  max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "TgGVMWq_GT7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "5r5-PISrGtKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XLNet<br>\n",
        "\n",
        "XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order.<br>\n",
        "\n",
        "Due to the difficulty of training a fully auto-regressive model over various factorization order, XLNet is pretrained using only a sub-set of the output tokens as target which are selected with the target_mapping input.<br>\n",
        "\n",
        "XLNet is one of the few models that has no sequence length limit."
      ],
      "metadata": {
        "id": "5HLje1xY2zUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xlnet_generator = pipeline(\"text-generation\", model=\"xlnet-base-cased\", tokenizer=\"xlnet-base-cased\")\n",
        "print(xlnet_generator(\"Hello, I'm a language model,\"))"
      ],
      "metadata": {
        "id": "1rqNVm714dNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8VG3NCLQCf8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we copy and paste the same text into three different models. <br>\n",
        "Compare the outputs and the length of time for each model"
      ],
      "metadata": {
        "id": "N_aPzOHbt0W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete the text:<br>\n",
        "**Copy and paste the following text into the model prompt**<br>\n",
        "The unicorns had seemed to know each other almost as well as they did common humans. The study was published in Science Translational Medicine on May 6. What's more, researchers found that five percent of the unicorns recognized each other well. The study team thinks this might translate into a future where humans would be able to communicate more clearly with those known as super Unicorns. And if we're going to move ahead with that future, we've got to do it at least a"
      ],
      "metadata": {
        "id": "rgotMIsklEZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R2XPXXAo4UKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-2**"
      ],
      "metadata": {
        "id": "C1uSgtt3Clmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py \\\n",
        "        --model_type=gpt2 --length=100 --model_name_or_path=gpt2"
      ],
      "metadata": {
        "id": "Tdt53Wtjk4pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample of Completed text from GPT2\n",
        "little faster than when unicorns first entered human speech.<br>\n",
        "\n",
        "This is nice news to hear.<br>\n",
        "\n",
        "What do you think about Devesque's research? Is it worth it? Tell us what you think in the comments below or tweet @TheCultula_.<|endoftext|>"
      ],
      "metadata": {
        "id": "03nKEcy-n_jX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5ULALoPOozRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample of Completed text from XLNet"
      ],
      "metadata": {
        "id": "Y246FAGknPP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py  \\\n",
        "    --model_type=xlnet \\\n",
        "    --length=100 \\\n",
        "    --model_name_or_path=xlnet-base-cased"
      ],
      "metadata": {
        "id": "I0oLwxASmzAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bit first.<eop> August 26, 2017In Singapore, the White Party spent the second day on their frontline road. The White Party, preferably the White House, spent the first day in a military parking garage in North Galleria, Washington, a two-day military \"preview session.\" The Democratic Party of Russia, representing the coalition with the White House, spent the first day in a military parking garage, a parking yard in East North Galleria, a parking garage in North\n"
      ],
      "metadata": {
        "id": "tG1umn7coXaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Gn_gS4RWsEXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample of Completed text from transformer-XL"
      ],
      "metadata": {
        "id": "fEDDkiQ2sFL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll see a message about model variables, for now we can ignore them. <br>\n",
        "**You'll have to be patient for the text completion - this can take a few minutes.**"
      ],
      "metadata": {
        "id": "t34BUidQuBkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py  \\\n",
        "    --model_type=transfo-xl \\\n",
        "    --length=100 \\\n",
        "    --model_name_or_path=transfo-xl-wt103"
      ],
      "metadata": {
        "id": "HDu7bE_Npa_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Completed by transformer-XL\n",
        "dozen times. For example, if you lose your head or your body, then you can't fight as a horse or lead a more manly life. There's always a chance of something changing. Today, we're going to do it every day, and that's exactly where you want to go. The knight is going to stop hunting, and he's going to allow a unicorn to come down. The knight will be so beautiful that he'll start stalking you until you've killed him. <eos>"
      ],
      "metadata": {
        "id": "VHGMxA57p3Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UNRff3aarFaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**<br>\n",
        "Open the file: /content/pytorch-transformers/examples/pytorch/text-generation/run_generation.py\n",
        "\n",
        "You'll see text used pad short input sequences.\n",
        "Change the text - search on Wikipedia, Reddit, or use any text that is at least 150 words long. <br>\n",
        "\n",
        "Re-run the notbook and see how the text changes."
      ],
      "metadata": {
        "id": "6L0ht1HsrG1Z"
      }
    }
  ]
}