{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOABqMO1HjGRFj+cVykf6FZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/1_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/huggingface/notebooks/blob/main/transformers_doc/quicktour.ipynb"
      ],
      "metadata": {
        "id": "1JPpj5BMtv6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick tour<br>\n",
        "Get up and running with 🤗 Transformers! Start using the pipeline() for rapid inference, and quickly load a pretrained model and tokenizer with an AutoClass to solve your text, vision or audio task."
      ],
      "metadata": {
        "id": "-xwrYtrOt-s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "WRQ0c4IVmIeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install the transformers libraries and datasets**<br>\n",
        "\n",
        "Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks."
      ],
      "metadata": {
        "id": "bnyWBCcZ4Avr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk5inHQWjOWP"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline<br>\n",
        "pipeline() is the easiest way to use a pretrained model for a given task.<br>\n",
        "These pipelines are objects that abstract most of the complex code from the library"
      ],
      "metadata": {
        "id": "et8uZcU1uKsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline() supports many common tasks out-of-the-box:\n",
        "\n",
        "**Text:**\n",
        "\n",
        ">-**Sentiment analysis**: classify the polarity of a given text.<br>\n",
        "-**Text generation (in English)**: generate text from a given input.<br>\n",
        "-**Name entity recognition (NER)**: label each word with the entity it represents (person, date, location, etc.).<br>\n",
        "-**Question answering**: extract the answer from the context, given some context and a question.<br>\n",
        "-**Fill-mask**: fill in the blank given a text with masked words.<br>\n",
        "-Summarization: generate a summary of a long sequence of text or document.<br>\n",
        "-**Translation**: translate text into another language.<br>\n",
        "-**Feature extraction**: create a tensor representation of the text.<br>\n",
        "-**Conversational**<br>\n",
        "-**Summarization**<br>\n",
        "-**Text Classification**<br>\n",
        "-**Text Generation**<br>\n",
        "-**Text2Text Generation**<br>\n",
        "-**ZeroShot Classification**<br>\n"
      ],
      "metadata": {
        "id": "aP2NR0tbuPj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Image:**\n",
        "\n",
        ">-**Image classification**: classify an image.<br>\n",
        "-**Image segmentation**: classify every pixel in an image.<br>\n",
        "-**Object detection**: detect objects within an image.<br><br>\n",
        "\n",
        "**Audio:**\n",
        "\n",
        ">**-Audio classification**: assign a label to a given segment of audio.<br>\n",
        "**-Automatic speech recognition (ASR)**: transcribe audio data into text.<br>"
      ],
      "metadata": {
        "id": "pc8J221X5BN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline usage<br>\n",
        "In the following example, you will use the pipeline() for sentiment analysis."
      ],
      "metadata": {
        "id": "hYexlN2bu75j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "qvgIqoPZ6bwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unless specified, the code should work in both PyTorch and TensorFlow\n",
        "!pip install torch\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "mgrQw6MLu3tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the sentiment analysis pipeline, also known as the text classification pipeline<br>\n",
        "<br>\n",
        "Classes are: <br>\n",
        ">positive<br>\n",
        "negative<br>\n",
        "neutral"
      ],
      "metadata": {
        "id": "juEDmQuI6d38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use sentiment text analysis for:\n",
        "\n",
        "- Analyze social media mentions to understand how people are talking about your brand vs your competitors.<br>\n",
        "- Analyze feedback from surveys and product reviews to quickly get insights into what your customers like and dislike about your product.<br>\n",
        "- Analyze incoming support tickets in real-time to detect angry customers and act accordingly to prevent churn.<br>"
      ],
      "metadata": {
        "id": "62uvpb8F8QyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more than 215 sentiment analysis models publicly available on the Hub and integrating them with Python just takes 5 lines of code.<br>\n",
        "In this instance we are using the default model.<br><br>\n",
        "**DistilBERT base uncased finetuned SST-2**<br>\n",
        "This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7)."
      ],
      "metadata": {
        "id": "cWPG-Pv78gd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will output a label and score for each input<br>\n",
        ">[{'label': 'POSITIVE', 'score': 0.9998},<br>\n",
        " {'label': 'NEGATIVE', 'score': 0.9991}]"
      ],
      "metadata": {
        "id": "goo7MCch9Ksy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "hGgqb_m1jgr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis. Now you can use the classifier on your target text:\n",
        "\n"
      ],
      "metadata": {
        "id": "_vtRd06PvFPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"I don't know if I like eel, sometimes I do.\")"
      ],
      "metadata": {
        "id": "Vqy1cUSujjr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more than one sentence, pass a list of sentences to the pipeline() which returns a list of dictionaries:\n",
        "\n"
      ],
      "metadata": {
        "id": "Yu0h4NZ1vJL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = classifier([\"I went to the park expecting it to be crowded, it wasn't!\", \n",
        "                      \"This park has nice views, but it had a lot of litter\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "metadata": {
        "id": "iZHMgURojmrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ues-5TtwJpKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline() can also iterate over an entire dataset. "
      ],
      "metadata": {
        "id": "iizkE2APvRoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a pipeline() with the task you want to solve for and the model you want to use."
      ],
      "metadata": {
        "id": "p8p8xa4AwI-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")"
      ],
      "metadata": {
        "id": "ad6B8cQkjmtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load a dataset (see the 🤗 Datasets Quick Start for more details) you'd like to iterate over. For example, let's load the MInDS-14 dataset:"
      ],
      "metadata": {
        "id": "hpealt_twLlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets[audio]"
      ],
      "metadata": {
        "id": "bFVDEv9PAxiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio"
      ],
      "metadata": {
        "id": "a85svfxrBPk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset requires a connection to dropbox. <br>\n",
        "Sometimes the connection is not successful. <bR>\n",
        "You may need to try several times"
      ],
      "metadata": {
        "id": "drQu7s7luesI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
      ],
      "metadata": {
        "id": "f9E0VKeDju15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "McohtWgCVgCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "-8C1nLi7Vlzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to make sure that the sampling rate of the dataset matches the sampling rate facebook/wav2vec2-base-960h was trained on."
      ],
      "metadata": {
        "id": "tgKGyQ06wPVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate)) "
      ],
      "metadata": {
        "id": "TV7nZ_a8ju4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audio files are automatically loaded and resampled when calling the \"audio\" column. Let's extract the raw waveform arrays of the first 4 samples and pass it as a list to the pipeline:\n",
        "\n"
      ],
      "metadata": {
        "id": "G9RtMpJQwS3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = speech_recognizer(dataset[:4][\"audio\"])\n",
        "#print([d[\"text\"] for d in result])\n",
        "for d in result:\n",
        "  print(d[\"text\"])"
      ],
      "metadata": {
        "id": "dPXXmgH8tcr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pWek00zcSCbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's listen to the speech and compare it to the text translation.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7fvZ6OdYvfbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sounddevice==0.4.4\n",
        "!pip install -q mediapipe==0.10.0"
      ],
      "metadata": {
        "id": "p86N5DuhRo76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O classifier.tflite -q https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite"
      ],
      "metadata": {
        "id": "OJeOKNXnR0sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib"
      ],
      "metadata": {
        "id": "1heXvVFBR2KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the file pathname from the printout of the details of the dataset. <br>\n",
        "The audio recording is a .wav file"
      ],
      "metadata": {
        "id": "VGX88cH8wFWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "5duK77S9wTte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "file_name = '/root/.cache/huggingface/datasets/downloads/extracted/a19fbc5032eacf25eab0097832db7b7f022b42104fbad6bd5765527704a428b9/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav'\n",
        "display(Audio(file_name, autoplay=False))"
      ],
      "metadata": {
        "id": "v-0CjsSdR8yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = speech_recognizer(dataset[0][\"audio\"])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "jlfEQTvmxymM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bDXPQT1uGhjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use another model and tokenizer in the pipeline<br>\n",
        "\n",
        "The pipeline() can accommodate any model from the Model Hub, making it easy to adapt the pipeline() for other use-cases. For example, if you'd like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. "
      ],
      "metadata": {
        "id": "UrlZv6Mzwfsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
      ],
      "metadata": {
        "id": "EbnJACFgtoAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and it's associated tokenizer (more on an AutoClass below):"
      ],
      "metadata": {
        "id": "C6LTYrFlw0On"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokienizer exmaple:**\n",
        "Consider the sentence: “Never give up”.\n",
        "\n",
        "The most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in<br>\n",
        "3 tokens – Never-give-up. <br>\n",
        "As each token is a word, it becomes an example of Word tokenization."
      ],
      "metadata": {
        "id": "zTFGHZwlymaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1h6sb1TBtqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "6Kg03CHMy3bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can specify the model and tokenizer in the pipeline(), and apply the classifier on your target text:"
      ],
      "metadata": {
        "id": "lpvOhXhlw-zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "cfZozWScttM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "classifier(\"This is a terribly noisy place\")"
      ],
      "metadata": {
        "id": "2b4ByJhOtzx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This is a wonderful place\")"
      ],
      "metadata": {
        "id": "PPCk4iDOzRNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This is a wonderful cafe but very noisy\")"
      ],
      "metadata": {
        "id": "oZTq5sDpzVJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you can't find a model for your use-case, you will need to fine-tune a pretrained model on your data. "
      ],
      "metadata": {
        "id": "o4rBFagsxCd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_e4AkJGvxWEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoClass"
      ],
      "metadata": {
        "id": "e2axBbLV036u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under the hood, the AutoModelForSequenceClassification and AutoTokenizer classes work together to power the pipeline(). An AutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate AutoClass for your task and it's associated tokenizer with AutoTokenizer.\n",
        "\n",
        "Let's return to our example and see how you can use the AutoClass to replicate the results of the pipeline()."
      ],
      "metadata": {
        "id": "MEf7zS-X0-NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoTokenizer<br><br>\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called tokens. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization here). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
        "\n",
        "Load a tokenizer with AutoTokenizer:"
      ],
      "metadata": {
        "id": "7q8rbQdg0_oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2vNwQi4t1F_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's vocabulary.\n",
        "\n",
        "Pass your text to the tokenizer:"
      ],
      "metadata": {
        "id": "B1wpQjDq1Iqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "VSIIe7lL18f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer will return a dictionary containing:\n",
        "\n",
        "input_ids: numerical representions of your tokens.\n",
        "atttention_mask: indicates which tokens should be attended to.\n",
        "Just like the pipeline(), the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"
      ],
      "metadata": {
        "id": "_se_6SWl1I0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "pt_batch = tokenizer(\n",
        "    [\"I am not happy with this tour and the day's schedule\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")    "
      ],
      "metadata": {
        "id": "c5GCfXJi2BAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_batch"
      ],
      "metadata": {
        "id": "AXlXIq1JzvXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TensorFlow\n",
        "tf_batch = tokenizer(\n",
        "    [\"I am not happy with this tour and the day's schedule\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"tf\",\n",
        ")"
      ],
      "metadata": {
        "id": "6-nWQmjG2Bb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_batch"
      ],
      "metadata": {
        "id": "EQNnDisB0FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the preprocessing tutorial for more details about tokenization."
      ],
      "metadata": {
        "id": "X57Dwlxp2IMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoModel<br>\n",
        "🤗 Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. The only difference is selecting the correct AutoModel for the task. Since you are doing text - or sequence - classification, load AutoModelForSequenceClassification:"
      ],
      "metadata": {
        "id": "n1i-zl65xXuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "rXWuEkwIxyWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "aA4B0Z84xcfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the task summary for which AutoModel class to use for which task.\n",
        "\n",
        "Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:"
      ],
      "metadata": {
        "id": "_GRrgqkzxf9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_outputs = pt_model(**pt_batch)"
      ],
      "metadata": {
        "id": "R1TKdOkAxhSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs the final activations in the logits attribute. Apply the softmax function to the logits to retrieve the probabilities:"
      ],
      "metadata": {
        "id": "pTcyl0jo2OXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
        "print(pt_predictions)"
      ],
      "metadata": {
        "id": "Ba9dfrrs2PH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Transformers provides a simple and unified way to load pretrained instances. This means you can load an TFAutoModel like you would load an AutoTokenizer. The only difference is selecting the correct TFAutoModel for the task. Since you are doing text - or sequence - classification, load TFAutoModelForSequenceClassification:"
      ],
      "metadata": {
        "id": "ZoYynGPA2TOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "L0wMOooT2VOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the task summary for which AutoModel class to use for which task.\n",
        "\n",
        "Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"
      ],
      "metadata": {
        "id": "jIUKB0Sd2YRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_outputs = tf_model(tf_batch)\n"
      ],
      "metadata": {
        "id": "5C-zF2iZ2bmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs the final activations in the logits attribute. Apply the softmax function to the logits to retrieve the probabilities:\n",
        "\n"
      ],
      "metadata": {
        "id": "hZ8yE5hl2eG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
        "tf_predictions"
      ],
      "metadata": {
        "id": "xXy5F57J2hpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All 🤗 Transformers models (PyTorch or TensorFlow) outputs the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.\n",
        "\n",
        "Models are a standard torch.nn.Module or a tf.keras.Model so you can use them in your usual training loop. However, to make things easier, 🤗 Transformers provides a Trainer class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the fit method from Keras. Refer to the training tutorial for more details.\n",
        "\n",
        "🤗 Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are None are ignored.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xVg_hH12lHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "X2ox-Abp21lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save a model<br>\n",
        "Once your model is fine-tuned, you can save it with its tokenizer using PreTrainedModel.save_pretrained():\n",
        "\n"
      ],
      "metadata": {
        "id": "JoOi9Lhu22yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_save_directory = \"./pt_save_pretrained\"\n",
        "tokenizer.save_pretrained(pt_save_directory)\n",
        "pt_model.save_pretrained(pt_save_directory)"
      ],
      "metadata": {
        "id": "m_0sU5zG27hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are ready to use the model again, reload it with PreTrainedModel.from_pretrained():"
      ],
      "metadata": {
        "id": "uPfh9uNP2-MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n"
      ],
      "metadata": {
        "id": "m1x4nkpD3AHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your model is fine-tuned, you can save it with its tokenizer using TFPreTrainedModel.save_pretrained():"
      ],
      "metadata": {
        "id": "v8ujHri03CgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_save_directory = \"./tf_save_pretrained\"\n",
        "tokenizer.save_pretrained(tf_save_directory)\n",
        "tf_model.save_pretrained(tf_save_directory)\n"
      ],
      "metadata": {
        "id": "IJnrzn1f3DMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are ready to use the model again, reload it with TFPreTrainedModel.from_pretrained():"
      ],
      "metadata": {
        "id": "dvSjck0t3HCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n"
      ],
      "metadata": {
        "id": "EpfmXeQD3JCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from_pt or from_tf parameter can convert the model from one framework to the other:"
      ],
      "metadata": {
        "id": "0V01jt1M3LWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)"
      ],
      "metadata": {
        "id": "XSiP1QnE3MEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)"
      ],
      "metadata": {
        "id": "tWl_h92A3aX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}