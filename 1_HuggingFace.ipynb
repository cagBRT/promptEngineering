{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNG55AMlPqe52XOP85ohh9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/1_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick tour<br>\n",
        "In this notebook we use the HuggingFace datasets, pipelines, and models to do the folliwing tasks: \n",
        "1. Sentiment text analysis\n",
        "2. Use the autoTokenizer and autoModel\n",
        "3. Save a model"
      ],
      "metadata": {
        "id": "-xwrYtrOt-s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "WRQ0c4IVmIeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install the transformers libraries and datasets**<br>\n",
        "\n",
        "Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks."
      ],
      "metadata": {
        "id": "bnyWBCcZ4Avr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk5inHQWjOWP"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline<br>\n",
        "pipeline() is the easiest way to use a pretrained model for a given task.<br>\n",
        "These pipelines are objects that abstract most of the complex code from the library"
      ],
      "metadata": {
        "id": "et8uZcU1uKsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline() supports many common tasks out-of-the-box:\n",
        "\n",
        "**Text:**\n",
        "\n",
        ">-**Sentiment analysis**: classify the polarity of a given text.<br>\n",
        "-**Text generation (in English)**: generate text from a given input.<br>\n",
        "-**Name entity recognition (NER)**: label each word with the entity it represents (person, date, location, etc.).<br>\n",
        "-**Question answering**: extract the answer from the context, given some context and a question.<br>\n",
        "-**Fill-mask**: fill in the blank given a text with masked words.<br>\n",
        "-Summarization: generate a summary of a long sequence of text or document.<br>\n",
        "-**Translation**: translate text into another language.<br>\n",
        "-**Feature extraction**: create a tensor representation of the text.<br>\n",
        "-**Conversational**<br>\n",
        "-**Summarization**<br>\n",
        "-**Text Classification**<br>\n",
        "-**Text Generation**<br>\n",
        "-**Text2Text Generation**<br>\n",
        "-**ZeroShot Classification**<br>\n"
      ],
      "metadata": {
        "id": "aP2NR0tbuPj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Image:**\n",
        "\n",
        ">-**Image classification**: classify an image.<br>\n",
        "-**Image segmentation**: classify every pixel in an image.<br>\n",
        "-**Object detection**: detect objects within an image.<br><br>\n",
        "\n",
        "**Audio:**\n",
        "\n",
        ">**-Audio classification**: assign a label to a given segment of audio.<br>\n",
        "**-Automatic speech recognition (ASR)**: transcribe audio data into text.<br>"
      ],
      "metadata": {
        "id": "pc8J221X5BN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline usage<br>\n",
        "In the following example, you will use the pipeline() for sentiment analysis."
      ],
      "metadata": {
        "id": "hYexlN2bu75j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "qvgIqoPZ6bwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "id": "D1N2zIAJlyBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unless specified, the code should work in both PyTorch and TensorFlow\n",
        "!pip install torch\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "mgrQw6MLu3tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the sentiment analysis pipeline, also known as the text classification pipeline<br>\n",
        "<br>\n",
        "Classes are: <br>\n",
        ">positive<br>\n",
        "negative<br>\n",
        "neutral"
      ],
      "metadata": {
        "id": "juEDmQuI6d38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use sentiment text analysis for:\n",
        "\n",
        "- Analyze social media mentions to understand how people are talking about your brand vs your competitors.<br>\n",
        "- Analyze feedback from surveys and product reviews to quickly get insights into what your customers like and dislike about your product.<br>\n",
        "- Analyze incoming support tickets in real-time to detect angry customers and act accordingly to prevent churn.<br>"
      ],
      "metadata": {
        "id": "62uvpb8F8QyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more than 215 sentiment analysis models publicly available on the Hub and integrating them with Python just takes 5 lines of code.<br>\n",
        "In this instance we are using the default model.<br><br>\n",
        "**DistilBERT base uncased finetuned SST-2**<br>\n",
        "This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7)."
      ],
      "metadata": {
        "id": "cWPG-Pv78gd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will output a label and score for each input<br>\n",
        ">[{'label': 'POSITIVE', 'score': 0.9998},<br>\n",
        " {'label': 'NEGATIVE', 'score': 0.9991}]"
      ],
      "metadata": {
        "id": "goo7MCch9Ksy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "hGgqb_m1jgr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis. Now you can use the classifier on your target text:\n",
        "\n"
      ],
      "metadata": {
        "id": "_vtRd06PvFPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"I don't know if I like eel, sometimes I do, sometimes I don't.\")"
      ],
      "metadata": {
        "id": "Vqy1cUSujjr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment:** <br>\n",
        "Enter other 'reviews' and see how the model performs."
      ],
      "metadata": {
        "id": "gHGfgfRnl_A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more than one sentence, pass a list of sentences to the pipeline() which returns a list of dictionaries:\n",
        "\n"
      ],
      "metadata": {
        "id": "Yu0h4NZ1vJL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = classifier([\"I went to the park expecting it to be crowded, it wasn't!\", \n",
        "                      \"This park has nice views, but it had a lot of litter\",\n",
        "                      \"The park has a lot of grass and trees, so it is quite relaxing\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "metadata": {
        "id": "iZHMgURojmrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1:**<br>\n",
        "Put in a string of sentences and see how the model performs. "
      ],
      "metadata": {
        "id": "ndLZo46HmRqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ues-5TtwJpKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use another model and tokenizer in the pipeline<br>\n",
        "\n",
        "The pipeline() can accommodate any model from the Model Hub.<br>\n",
        "If you'd like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. "
      ],
      "metadata": {
        "id": "UrlZv6Mzwfsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
      ],
      "metadata": {
        "id": "EbnJACFgtoAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and it's associated tokenizer (more on an AutoClass below):"
      ],
      "metadata": {
        "id": "C6LTYrFlw0On"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokienizer exmaple:**\n",
        "Consider the sentence: “Never give up”.\n",
        "\n",
        "The most common way of forming tokens is based on space. Assuming space as a delimiter, the tokenization of the sentence results in<br>\n",
        ">3 tokens = Never-give-up. <br>\n",
        "As each token is a word, it becomes an example of Word tokenization."
      ],
      "metadata": {
        "id": "zTFGHZwlymaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "1h6sb1TBtqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "6Kg03CHMy3bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can specify the model and tokenizer in the pipeline(), and apply the classifier on your target text:"
      ],
      "metadata": {
        "id": "lpvOhXhlw-zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "cfZozWScttM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "classifier(\"This is a terribly noisy place\")"
      ],
      "metadata": {
        "id": "2b4ByJhOtzx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This is a wonderful place\")"
      ],
      "metadata": {
        "id": "PPCk4iDOzRNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"This is a wonderful cafe but very noisy\")"
      ],
      "metadata": {
        "id": "oZTq5sDpzVJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you can't find a model for your use-case, you will need to fine-tune a pretrained model on your data. "
      ],
      "metadata": {
        "id": "o4rBFagsxCd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_e4AkJGvxWEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoClass"
      ],
      "metadata": {
        "id": "e2axBbLV036u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under the hood, the AutoModelForSequenceClassification and AutoTokenizer classes work together to power the pipeline(). An AutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate AutoClass for your task and it's associated tokenizer with AutoTokenizer.\n"
      ],
      "metadata": {
        "id": "MEf7zS-X0-NF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoTokenizer<br><br>\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called tokens. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization here). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
        "\n",
        "Load a tokenizer with AutoTokenizer:"
      ],
      "metadata": {
        "id": "7q8rbQdg0_oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "2vNwQi4t1F_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's vocabulary.\n",
        "\n",
        "Pass your text to the tokenizer:"
      ],
      "metadata": {
        "id": "B1wpQjDq1Iqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer(\"Monty Python is a great act, Python is a great language, Pythons are not good pets\")\n",
        "print(encoding)"
      ],
      "metadata": {
        "id": "VSIIe7lL18f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the tokienizer set all the Python words to 43611. <br>\n",
        "All three uses of the word \"Python\" are different and this difference is important to correctly understand the input sequence. "
      ],
      "metadata": {
        "id": "kloiHIJYnRdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2:** <br>\n",
        "Write different sentences and see how the model tokenizes them. "
      ],
      "metadata": {
        "id": "kGOa1au_mwZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer will return a dictionary containing:\n",
        "\n",
        "input_ids: numerical representions of your tokens.\n",
        "atttention_mask: indicates which tokens should be attended to.\n",
        "Just like the pipeline(), the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"
      ],
      "metadata": {
        "id": "_se_6SWl1I0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "#pt_batch = tokenizer(\n",
        "#    [\"I am not happy with this tour and the day's schedule\"],\n",
        "#    padding=True,\n",
        "#    truncation=True,\n",
        "#    max_length=512,\n",
        "#    return_tensors=\"pt\",\n",
        "#)    "
      ],
      "metadata": {
        "id": "c5GCfXJi2BAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pt_batch"
      ],
      "metadata": {
        "id": "AXlXIq1JzvXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TensorFlow\n",
        "tf_batch = tokenizer(\n",
        "    [\"I am not happy with this tour and the day's schedule\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"tf\",\n",
        ")"
      ],
      "metadata": {
        "id": "6-nWQmjG2Bb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_batch"
      ],
      "metadata": {
        "id": "EQNnDisB0FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoModel<br>\n",
        "Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. <br>\n",
        "The only difference is selecting the correct AutoModel for the task. Since you are doing text - or sequence - classification, load AutoModelForSequenceClassification.\n",
        "<br><br>\n",
        "**SentencePiece** is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training."
      ],
      "metadata": {
        "id": "n1i-zl65xXuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "rXWuEkwIxyWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "#model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "#pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "aA4B0Z84xcfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the task summary for which AutoModel class to use for which task.\n",
        "\n",
        "Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:"
      ],
      "metadata": {
        "id": "_GRrgqkzxf9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pt_outputs = pt_model(**pt_batch)\n",
        "#pt_outputs"
      ],
      "metadata": {
        "id": "R1TKdOkAxhSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model predicts the sentiment of text as the number of stars 1-5. <br>\n",
        "The logits (below) are between 0-1. <br>\n"
      ],
      "metadata": {
        "id": "brtviZnNYuEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs the final activations in the logits attribute. Apply the softmax function to the logits to retrieve the probabilities:"
      ],
      "metadata": {
        "id": "pTcyl0jo2OXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn is a library that contains difference classes that help you build neural network models. "
      ],
      "metadata": {
        "id": "FwRz2f_uZs3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from torch import nn\n",
        "\n",
        "#pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
        "#print(pt_predictions)"
      ],
      "metadata": {
        "id": "Ba9dfrrs2PH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Transformers provides a simple and unified way to load pretrained instances. This means you can load an TFAutoModel like you would load an AutoTokenizer. The only difference is selecting the correct TFAutoModel for the task. Since you are doing text - or sequence - classification, load TFAutoModelForSequenceClassification:"
      ],
      "metadata": {
        "id": "ZoYynGPA2TOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "L0wMOooT2VOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the task summary for which AutoModel class to use for which task.\n",
        "\n",
        "Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"
      ],
      "metadata": {
        "id": "jIUKB0Sd2YRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_outputs = tf_model(tf_batch) #I am not happy with this tour and the day's schedule\""
      ],
      "metadata": {
        "id": "5C-zF2iZ2bmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What review is given: 1,2,3,4,or 5 stars"
      ],
      "metadata": {
        "id": "1CBBYkkTo9pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_outputs"
      ],
      "metadata": {
        "id": "jTMpFk0wozdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs the final activations in the logits attribute. Apply the softmax function to the logits to retrieve the probabilities:\n",
        "\n"
      ],
      "metadata": {
        "id": "hZ8yE5hl2eG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
        "tf_predictions"
      ],
      "metadata": {
        "id": "xXy5F57J2hpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**: <br>\n",
        "Try different reviews, how does the model perform?"
      ],
      "metadata": {
        "id": "K7oIPvxQpSuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment: \n",
        "tf_batch = tokenizer(\n",
        "    [\"I am not happy with this tour and the day's schedule\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"tf\",\n",
        ")\n",
        "tf_outputs = tf_model(tf_batch)"
      ],
      "metadata": {
        "id": "EhzSKB41pZGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assignment: \n",
        "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
        "tf_predictions"
      ],
      "metadata": {
        "id": "hHBycf6Rpm4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "m9Ob90jEpoI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All HuggingFace Transformers models (PyTorch or TensorFlow) outputs the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.\n",
        "\n",
        "Models are a standard torch.nn.Module or a tf.keras.Model so you can use them in your usual training loop. However, to make things easier, HuggingFace Transformers provides a Trainer class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the fit method from Keras. Refer to the training tutorial for more details.\n",
        "\n",
        "HuggingFace Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are None are ignored.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xVg_hH12lHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "X2ox-Abp21lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save a model<br>\n",
        "Once your model is fine-tuned, you can save it with its tokenizer using PreTrainedModel.save_pretrained():\n",
        "\n"
      ],
      "metadata": {
        "id": "JoOi9Lhu22yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pt_save_directory = \"./pt_save_pretrained\"\n",
        "#tokenizer.save_pretrained(pt_save_directory)\n",
        "#pt_model.save_pretrained(pt_save_directory)"
      ],
      "metadata": {
        "id": "m_0sU5zG27hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are ready to use the model again, reload it with PreTrainedModel.from_pretrained():"
      ],
      "metadata": {
        "id": "uPfh9uNP2-MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
      ],
      "metadata": {
        "id": "m1x4nkpD3AHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your model is fine-tuned, you can save it with its tokenizer using TFPreTrainedModel.save_pretrained():"
      ],
      "metadata": {
        "id": "v8ujHri03CgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_save_directory = \"./tf_save_pretrained\"\n",
        "#tokenizer.save_pretrained(tf_save_directory)\n",
        "#tf_model.save_pretrained(tf_save_directory)"
      ],
      "metadata": {
        "id": "IJnrzn1f3DMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are ready to use the model again, reload it with TFPreTrainedModel.from_pretrained():"
      ],
      "metadata": {
        "id": "dvSjck0t3HCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
      ],
      "metadata": {
        "id": "EpfmXeQD3JCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from_pt or from_tf parameter can convert the model from one framework to the other:"
      ],
      "metadata": {
        "id": "0V01jt1M3LWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import AutoModel\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
        "#pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)"
      ],
      "metadata": {
        "id": "XSiP1QnE3MEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import TFAutoModel\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
        "#tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)"
      ],
      "metadata": {
        "id": "tWl_h92A3aX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}