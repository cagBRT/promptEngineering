{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNOH7S3s5eyfCpHDdWRqiy3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational GPT2"
      ],
      "metadata": {
        "id": "ORL7f0h7zLeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/promptEngineering.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "aHXLTK8M0sYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q transformers"
      ],
      "metadata": {
        "id": "gbihGtltsUsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "id": "VkUu_baXxi-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their model card:"
      ],
      "metadata": {
        "id": "pPwf1USg6y1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Because large-scale language models like GPT-2 do not distinguish fact from fiction, we donâ€™t support use-cases that require the generated text to be true.**\n",
        "\n",
        "Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case.<br><br>\n",
        "\n",
        "We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n",
        "\n"
      ],
      "metadata": {
        "id": "lS7RDG8Q62JY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model below will return the top five responses."
      ],
      "metadata": {
        "id": "Qd_qUq_fw6Hi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuFeJCORsPKf"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OpenAI team wanted to train this model on a corpus as large as possible. <br>\n",
        "To build it, they **scraped all the web pages from outbound links on Reddit which received at least 3 karma.**<br><br>Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia.<br>\n",
        "\n",
        "*The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released.*"
      ],
      "metadata": {
        "id": "mtf6i3rX7oRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import pipeline, set_seed\n",
        "#generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"The man worked as a\", max_length=10, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "SBOjm-NVsSMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you think some for the responses for \"The dog worked as a ...\""
      ],
      "metadata": {
        "id": "p2EVgXDkxT_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "generator(\"The dog worked as a\", max_length=10, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "ipEmjpFUstkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b-Wpy7whwcEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow GPT2 Model"
      ],
      "metadata": {
        "id": "D7wzYgM4xske"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2Model.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "a4b95RjZx2jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize input sequences**"
      ],
      "metadata": {
        "id": "XxL43QHfyPsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)\n",
        "#output"
      ],
      "metadata": {
        "id": "iVv52lOFwmtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational Models"
      ],
      "metadata": {
        "id": "txMfaqm5yYEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, Conversation\n",
        "converse = pipeline(\"conversational\")"
      ],
      "metadata": {
        "id": "mfxah4u7aEEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_1 = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
        "converse([conversation_1])"
      ],
      "metadata": {
        "id": "4viFMPchZ6Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_2 = Conversation(\"What's the last book you have read?\")\n",
        "converse([ conversation_2])"
      ],
      "metadata": {
        "id": "byjRrvQfaIpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an instance of microsoft/DialoGPT-medium trained on a game character, Joshua from The World Ends With You. The data comes from a Kaggle game script dataset. Chat with the model:\n",
        "\n"
      ],
      "metadata": {
        "id": "V2BAduAQbCjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "BzKkPE3Nytfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"gpt2-sizes-hyperparameters-3.png\", width=640)"
      ],
      "metadata": {
        "id": "MKEv6Hmc077n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. One key difference between the two is that GPT2, like traditional language models, outputs one token at a time."
      ],
      "metadata": {
        "id": "2LYJREVL1dUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\",padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
      ],
      "metadata": {
        "id": "LBCdcMCQd2q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "times=5\n",
        "# Let's chat for 5 lines\n",
        "for step in range(times):\n",
        "  # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "  new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "  # append the new user input tokens to the chat history\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "  # generated a response while limiting the total chat history to 1000 tokens,\n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "  # pretty print last ouput tokens from bot\n",
        "  print(\"AI: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "KEjBeu-ebt-4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}