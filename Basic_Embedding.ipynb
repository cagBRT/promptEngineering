{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOhKtYx5AcCVNC+oYMhVqFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Basic_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the libraries**"
      ],
      "metadata": {
        "id": "air90-7Pe-vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from keras.layers import TextVectorization, Embedding, Layer\n",
        "from tensorflow.data import Dataset\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TGFME10AsTO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decide on the output sequence length<br>\n",
        "- The vocabulary size<br>\n",
        "- The text you want to embed using the text vectorization layer from Keras"
      ],
      "metadata": {
        "id": "IoPRqskMgdNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequence_length = 5\n",
        "vocab_size = 15\n",
        "#note the sentences do not have punctuation\n",
        "sentences = [[\"I am a robot\"], [\"you are a robot\"], [\"you are not a robot in my mind\"]]\n",
        "sentence_data = Dataset.from_tensor_slices(sentences)"
      ],
      "metadata": {
        "id": "tV9XPIMHfN_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The text vectorization layer**<br>\n",
        "The text vectorization layer creates a dictionary of words and replaces each word with its corresponding index in the dictionary."
      ],
      "metadata": {
        "id": "Fq5YQdGRiRAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the layer is a tensor of shape:<br>\n",
        "\n",
        "(number of sentences, output sequence length)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Itfa0A-g_6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TextVectorization layer\n",
        "vectorize_layer = TextVectorization(\n",
        "                  output_sequence_length=output_sequence_length,\n",
        "                  max_tokens=vocab_size)\n",
        "# Train the layer to create a dictionary\n",
        "vectorize_layer.adapt(sentence_data)\n",
        "# Convert all sentences to tensors\n",
        "word_tensors = convert_to_tensor(sentences, dtype=tf.string)\n",
        "word_tensors"
      ],
      "metadata": {
        "id": "MXb7jJ47sLGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is set to the output sequence length equal to 5.<br>\n",
        "Either the text is padded or truncated"
      ],
      "metadata": {
        "id": "vnxUuV3ajMvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the word tensors to get vectorized phrases\n",
        "vectorized_words = vectorize_layer(word_tensors)\n",
        "print(sentences[0])\n",
        "vectorized_words[0]"
      ],
      "metadata": {
        "id": "BHAaFChns3U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1])\n",
        "vectorized_words[1]"
      ],
      "metadata": {
        "id": "nmU4rteKjIzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[2])\n",
        "vectorized_words[2]"
      ],
      "metadata": {
        "id": "ZDJ38zCMjhkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape is the number of sentences, output sequence length"
      ],
      "metadata": {
        "id": "9f05F_wjjxqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
        "print(\"Vectorized words: \", vectorized_words)"
      ],
      "metadata": {
        "id": "fAdG5sAfs-bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**<br>\n",
        "The Keras Embedding layer converts integers to dense vectors.<br>\n",
        "This layer maps these integers to random numbers, which are later tuned during the training phase. However, you also have the option to set the mapping to some predefined weight values.<br>\n",
        "\n",
        "To initialize this layer, you need to specify the maximum value of an integer to map, along with the length of the output sequence."
      ],
      "metadata": {
        "id": "eh_rsMqMtm8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_length = 6 #number of dimensions\n",
        "word_embedding_layer = Embedding(vocab_size, output_length)\n",
        "embedded_words = word_embedding_layer(vectorized_words)\n",
        "print(sentences[0],\"\\n\",vectorized_words[0], \"\\n\", embedded_words[0])"
      ],
      "metadata": {
        "id": "qKhPhm3otjQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time you run this code the embedded words will change. This is because the weights are randomly selected to beginn. Later they will be tuned. "
      ],
      "metadata": {
        "id": "9FvKsCvNkuv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1],\"\\n\",vectorized_words[1], \"\\n\", embedded_words[1])"
      ],
      "metadata": {
        "id": "dc5BecKLuJzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[2],\"\\n\",vectorized_words[2], \"\\n\", embedded_words[2])"
      ],
      "metadata": {
        "id": "tuRvpFqHkVGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Position encoding**<br>\n",
        "You also need the embeddings for the corresponding positions. The maximum positions correspond to the output sequence length of the TextVectorization layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "9w6GwyncuPfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position_embedding_layer = Embedding(output_sequence_length, output_length)\n",
        "position_indices = tf.range(output_sequence_length)\n",
        "print(position_indices)"
      ],
      "metadata": {
        "id": "HkkjCEpRuREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_indices = position_embedding_layer(position_indices)"
      ],
      "metadata": {
        "id": "CPLlrSf2vZPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output sequence length is five. "
      ],
      "metadata": {
        "id": "tZn5wlrTlS0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a transformer model, the final output is the sum of both the word embeddings and the position embeddings.<br>\n",
        "\n",
        "When you set up both embedding layers, you need to make sure that the output_length is the same for both."
      ],
      "metadata": {
        "id": "_btVmqtTmGwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\"index\",[i], \": \",embedded_indices[i])"
      ],
      "metadata": {
        "id": "iQ7NRBNavZZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_embedding = embedded_words + embedded_indices\n",
        "for i in range(len(sentences)):\n",
        "  print(sentences[i],\"\\n\" \"Final output: \",  final_output_embedding[i])\n",
        "  print(\"\\n\") \n",
        "#there are 5 rows - one for each word input\n",
        "#there are 6 columns, to match the output sequence size"
      ],
      "metadata": {
        "id": "rOvfq9Zgl9_I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}