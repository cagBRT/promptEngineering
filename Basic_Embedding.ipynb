{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO4AVO2K4HgxlbXNZ1QjihD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Basic_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A notebook to help with understanding basic embedding <br>\n"
      ],
      "metadata": {
        "id": "rRdIGhIDltg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook focuses on vectorizing text sequences and positional embedding"
      ],
      "metadata": {
        "id": "JHQNr-xU1UI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the libraries**"
      ],
      "metadata": {
        "id": "air90-7Pe-vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from keras.layers import TextVectorization, Embedding, Layer\n",
        "from tensorflow.data import Dataset\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TGFME10AsTO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Decide on the output sequence length<br>\n",
        "- The vocabulary size<br>\n",
        "- The text you want to embed using the text vectorization layer from Keras"
      ],
      "metadata": {
        "id": "IoPRqskMgdNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output sequence length is the length of the text sequence that will go to the transformer. <br>\n",
        "Vocabulary size is all the unique words in the training text sequences"
      ],
      "metadata": {
        "id": "vhIYUmarl1hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequence_length = 5\n",
        "vocab_size = 15\n",
        "#note the sentences do not have punctuation\n",
        "sentences = [[\"I am a robot\"], [\"you are a robot\"], [\"you are not a robot in my mind\"]]\n",
        "sentence_data = Dataset.from_tensor_slices(sentences)"
      ],
      "metadata": {
        "id": "tV9XPIMHfN_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The text vectorization layer**<br>\n",
        "The text vectorization layer creates a dictionary of words and replaces each word with its corresponding index in the dictionary."
      ],
      "metadata": {
        "id": "Fq5YQdGRiRAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the layer is a tensor of shape:<br>\n",
        "\n",
        "(number of sentences, output sequence length)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Itfa0A-g_6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TextVectorization layer\n",
        "vectorize_layer = TextVectorization(\n",
        "                  output_sequence_length=output_sequence_length,\n",
        "                  max_tokens=vocab_size)\n",
        "# Train the layer to create a dictionary\n",
        "vectorize_layer.adapt(sentence_data)\n",
        "# Convert all sentences to tensors\n",
        "word_tensors = convert_to_tensor(sentences, dtype=tf.string)\n",
        "word_tensors"
      ],
      "metadata": {
        "id": "MXb7jJ47sLGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is set to the output sequence length equal to 5.<br>\n",
        "The text might be padded or truncated"
      ],
      "metadata": {
        "id": "vnxUuV3ajMvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the word tensors to get vectorized phrases\n",
        "vectorized_words = vectorize_layer(word_tensors)\n",
        "print(sentences[0])\n",
        "vectorized_words[0]\n",
        "#the sequence is 4 words, add a pad to make it length 5"
      ],
      "metadata": {
        "id": "BHAaFChns3U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1])\n",
        "vectorized_words[1]\n",
        "#the sequence is 4 words, add a pad to make it length 5"
      ],
      "metadata": {
        "id": "nmU4rteKjIzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the following vectorization, the sentence is truncated to fit the length limit"
      ],
      "metadata": {
        "id": "7P_jkMCyhAUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[2])\n",
        "vectorized_words[2]\n",
        "#the sequence is 8 words, truncate to make it length 5"
      ],
      "metadata": {
        "id": "ZDJ38zCMjhkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape is the number of sentences, output sequence length<br>\n",
        "\n",
        ">robot=2<br>\n",
        "a=3<br>\n",
        "you=4<br>\n",
        "...\n"
      ],
      "metadata": {
        "id": "9f05F_wjjxqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
        "print(\"Vectorized words: \", vectorized_words)"
      ],
      "metadata": {
        "id": "fAdG5sAfs-bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1:**<br>\n",
        "Try changing the sentences, the number of vocabulary words, and  the output sequence length."
      ],
      "metadata": {
        "id": "5q6lrF8KujII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FdD7uJWvvUAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**<br>\n",
        "The Keras Embedding layer converts integers to dense vectors.<br>\n",
        "This layer maps these integers to random numbers, which are later tuned during the training phase. However, you also have the option to set the mapping to some predefined weight values.<br>\n",
        "\n",
        "To initialize this layer, you need to specify the maximum value of an integer to map, along with the length of the output sequence."
      ],
      "metadata": {
        "id": "eh_rsMqMtm8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_length = 6 #number of dimensions\n",
        "word_embedding_layer = Embedding(vocab_size, output_length)\n",
        "embedded_words = word_embedding_layer(vectorized_words)\n",
        "print(sentences[0],\"\\n\",vectorized_words[0], \"\\n\", embedded_words[0])"
      ],
      "metadata": {
        "id": "qKhPhm3otjQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note in the above tensor**, the sentence is padded with one zero, to make the sentence have a length of 5. <br>\n",
        "Also notice, there are six elements in the vectors, one for each dimension. <br>\n"
      ],
      "metadata": {
        "id": "8m4MprAwveNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time you run this code the embedded words will change. This is because the weights are randomly selected, later they will be tuned. <br>"
      ],
      "metadata": {
        "id": "9FvKsCvNkuv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[1],\"\\n\",vectorized_words[1], \"\\n\", embedded_words[1])"
      ],
      "metadata": {
        "id": "dc5BecKLuJzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[2],\"\\n\",vectorized_words[2], \"\\n\", embedded_words[2])"
      ],
      "metadata": {
        "id": "tuRvpFqHkVGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Position encoding**<br>\n",
        "You also need the embeddings for the corresponding positions.\n",
        "\n"
      ],
      "metadata": {
        "id": "9w6GwyncuPfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position_embedding_layer = Embedding(output_sequence_length, output_length)\n",
        "position_indices = tf.range(output_sequence_length)\n",
        "print(position_indices)"
      ],
      "metadata": {
        "id": "HkkjCEpRuREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_indices = position_embedding_layer(position_indices)"
      ],
      "metadata": {
        "id": "CPLlrSf2vZPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output sequence length is five."
      ],
      "metadata": {
        "id": "tZn5wlrTlS0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a transformer model, the final output is the sum of both the word embeddings and the position embeddings.<br>\n",
        "\n",
        "When you set up both embedding layers, you need to make sure that the output_length is the same for both."
      ],
      "metadata": {
        "id": "_btVmqtTmGwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(output_sequence_length):\n",
        "  print(\"index\",[i], \": \",embedded_indices[i])"
      ],
      "metadata": {
        "id": "iQ7NRBNavZZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final output embedding is the sum of the embedded words and the embedded indices"
      ],
      "metadata": {
        "id": "7N1c3BtIhtEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_embedding = embedded_words + embedded_indices"
      ],
      "metadata": {
        "id": "OM8BjwDQhsEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  print(sentences[i],\"\\n\" \"Final output: \",  final_output_embedding[i])\n",
        "  print(\"\\n\")\n",
        "#there are 5 rows - one for each word input\n",
        "#there are 6 columns, to match the output sequence size"
      ],
      "metadata": {
        "id": "rOvfq9Zgl9_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Using fixed weights**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fYmdpyh2oXft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Output of Positional Encoding Layer in Transformers\n",
        "In a transformer model, the final output is the sum of both the word embeddings and the position embeddings. Hence, when you set up both embedding layers, you need to make sure that the output_length is the same for both."
      ],
      "metadata": {
        "id": "Nf3DTlVQoY2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weights are initialized randomly and tuned during the training phase.\n"
      ],
      "metadata": {
        "id": "yG_FX0rKpMFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This example shows how you can subclass the Embedding layer to implement your own functionality. You can add more methods to it as you require."
      ],
      "metadata": {
        "id": "hVhApo3WqRW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You can use the PositionEmbeddingLayer gunction in Keras instead of writing your own\n",
        "class PositionEmbeddingLayer(Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
        "        super(PositionEmbeddingLayer, self).__init__(**kwargs)\n",
        "        self.word_embedding_layer = Embedding(\n",
        "            input_dim=vocab_size, output_dim=output_dim\n",
        "        )\n",
        "        self.position_embedding_layer = Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ],
      "metadata": {
        "id": "b8oSMQj6po0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_embedding_layer = PositionEmbeddingLayer(output_sequence_length,\n",
        "                                            vocab_size, output_length)\n",
        "embedded_layer_output = my_embedding_layer(vectorized_words)\n",
        "for i in range(len(sentences)):\n",
        "  print(sentences[i],\"Output from my_embedded_layer: \", embedded_layer_output[i], \"\\n\")"
      ],
      "metadata": {
        "id": "bxpZiYLxpvzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the above class creates an embedding layer that has trainable weights. Hence, the weights are initialized randomly and tuned in to the training phase."
      ],
      "metadata": {
        "id": "uYXqhjHEqr0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When specifying the Embedding layer, you need to provide the positional encoding matrix as weights along with trainable=False. Let’s create another positional embedding class that does exactly this."
      ],
      "metadata": {
        "id": "TDTAV9mFqs5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbeddingFixedWeights(Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
        "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
        "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)\n",
        "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)\n",
        "        self.word_embedding_layer = Embedding(\n",
        "            input_dim=vocab_size, output_dim=output_dim,\n",
        "            weights=[word_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "        self.position_embedding_layer = Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim,\n",
        "            weights=[position_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def get_position_encoding(self, seq_len, d, n=10000):\n",
        "        P = np.zeros((seq_len, d))\n",
        "        for k in range(seq_len):\n",
        "            for i in np.arange(int(d/2)):\n",
        "                denominator = np.power(n, 2*i/d)\n",
        "                P[k, 2*i] = np.sin(k/denominator)\n",
        "                P[k, 2*i+1] = np.cos(k/denominator)\n",
        "        return P\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ],
      "metadata": {
        "id": "FrZllPropYRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention models, also called attention mechanisms, are deep learning techniques used to provide an additional focus on a specific component. In deep learning, attention relates to focus on something in particular and note its specific importance"
      ],
      "metadata": {
        "id": "w8xUtCVkiKhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attnisallyouneed_embedding = PositionEmbeddingFixedWeights(output_sequence_length,\n",
        "                                            vocab_size, output_length)\n",
        "attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\n",
        "#print(\"Output from my_embedded_layer: \", attnisallyouneed_output)\n",
        "for i in range(len(sentences)):\n",
        "  print(sentences[i],\"Output from my_embedded_layer: \", embedded_layer_output[i], \"\\n\")"
      ],
      "metadata": {
        "id": "SVsPCtMtpbnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BJbPQfEboZ4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An example with fixed weights"
      ],
      "metadata": {
        "id": "PlZwM0ncocaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary is from two phrases: technical, wise<br>\n",
        "The sequence length in is 20 <br>\n",
        "The sequence length out is 50<br>"
      ],
      "metadata": {
        "id": "TgYrH5kcojIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_vocabulary = 200\n",
        "sequence_length = 20\n",
        "final_output_len = 50"
      ],
      "metadata": {
        "id": "aClLuYRDsOzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our corpus comes from these two phrases"
      ],
      "metadata": {
        "id": "Uc_G77-MoyoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "technical_phrase = \"to understand machine learning algorithms you need\" +\\\n",
        "                   \" to understand concepts such as gradient of a function \"+\\\n",
        "                   \"Hessians of a matrix and optimization etc\"\n",
        "wise_phrase = \"patrick henry said give me liberty or give me death \"+\\\n",
        "              \"when he addressed the second virginia convention in march\""
      ],
      "metadata": {
        "id": "5QQbKGLRsV1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the two phrases"
      ],
      "metadata": {
        "id": "VV-HwLNyo48f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_vectorization_layer = TextVectorization(\n",
        "                  output_sequence_length=sequence_length,\n",
        "                  max_tokens=total_vocabulary)\n",
        "# Learn the dictionary\n",
        "phrase_vectorization_layer.adapt([technical_phrase, wise_phrase])"
      ],
      "metadata": {
        "id": "mLy5YtEgpmlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all sentences to tensors\n",
        "phrase_tensors = convert_to_tensor([technical_phrase, wise_phrase],\n",
        "                                   dtype=tf.string)\n",
        "\n",
        "phrase_tensors\n",
        "#tech phrase = 22 words\n",
        "#wise phrase= 19 words"
      ],
      "metadata": {
        "id": "mSZEYgHWsX3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the word tensors to get vectorized phrases\n",
        "vectorized_phrases = phrase_vectorization_layer(phrase_tensors)\n",
        "vectorized_phrases\n",
        "#notice there is no BOS or EOS"
      ],
      "metadata": {
        "id": "euce2jRBsczT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_weights_embedding_layer = PositionEmbeddingLayer(sequence_length,\n",
        "                                                        total_vocabulary,\n",
        "                                                        final_output_len)\n",
        "fixed_weights_embedding_layer = PositionEmbeddingFixedWeights(sequence_length,\n",
        "                                                        total_vocabulary,\n",
        "                                                        final_output_len)\n",
        "random_embedding = random_weights_embedding_layer(vectorized_phrases)\n",
        "fixed_embedding = fixed_weights_embedding_layer(vectorized_phrases)"
      ],
      "metadata": {
        "id": "d9GqMOfYpTMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the sequence length is 20<br>\n"
      ],
      "metadata": {
        "id": "crkNvzcirsjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(technical_phrase,\"\\nOutput from my_embedded_layer: \", vectorized_phrases[0], \"\\n\")"
      ],
      "metadata": {
        "id": "9OwDIyZOrXA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The vectorized phrase**<br>\n",
        "3 - to <br>\n",
        "2  - understand<br>\n",
        "21 - machine <br>\n",
        "23 - learning <br>\n",
        "36 - algorithms <br>\n",
        "8 - you<br>\n",
        "18 - need<br>\n",
        "3  - to <br>\n",
        "2 - understand<br>\n",
        "33 - concepts<br>\n",
        "12 - such<br>\n",
        "34 - as<br>\n",
        "28 - gradient<br>\n",
        "4  - of<br>\n",
        "7 - a<br>\n",
        "29 - function<br>\n",
        "25 - Hessians<br>\n",
        "4  - of <br>\n",
        "7 - a <br>\n",
        "19 - matrix"
      ],
      "metadata": {
        "id": "zRWz_Nr0qVtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wise_phrase,\"\\nOutput from my_embedded_layer: \", vectorized_phrases[1], \"\\n\")"
      ],
      "metadata": {
        "id": "B9DuOZder_YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we take our vectorized text sequence and embed it with the position information"
      ],
      "metadata": {
        "id": "e2GefS3Zrqpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_embedding[0]"
      ],
      "metadata": {
        "id": "sVF_qDVWsFCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fig = plt.figure(figsize=(15, 5))\n",
        "#title = [\"Tech Phrase\", \"Wise Phrase\"]\n",
        "#for i in range(2):\n",
        "#    ax = plt.subplot(1, 2, 1+i)\n",
        "#    matrix = tf.reshape(random_embedding[i, :, :], (sequence_length, final_output_len))\n",
        "#    cax = ax.matshow(matrix)\n",
        "#    plt.gcf().colorbar(cax)\n",
        "#    plt.title(title[i], y=1.2)\n",
        "#fig.suptitle(\"Random Embedding\")\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "yMajhqFNp3Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fig = plt.figure(figsize=(15, 5))\n",
        "#title = [\"Tech Phrase\", \"Wise Phrase\"]\n",
        "#for i in range(2):\n",
        "#    ax = plt.subplot(1, 2, 1+i)\n",
        "#    matrix = tf.reshape(fixed_embedding[i, :, :], (sequence_length, final_output_len))\n",
        "#    cax = ax.matshow(matrix)\n",
        "#    plt.gcf().colorbar(cax)\n",
        "#    plt.title(title[i], y=1.2)\n",
        "#fig.suptitle(\"Fixed Weight Embedding from Attention is All You Need\")\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "kLjFu4Ldp8h-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}