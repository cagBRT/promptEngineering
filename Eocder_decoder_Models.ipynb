{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMvAB2jWy8oJFMjJya/f0Xp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Eocder_decoder_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook gives examples of implementing Encoder models"
      ],
      "metadata": {
        "id": "ovFhUGtN5U5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/promptEngineering.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "xnjoiN8b34zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "tv4-sNch3-jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets"
      ],
      "metadata": {
        "id": "KO2v7xZMp2JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "RQ15T5sw02Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "etpwEkpU0hG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import load_dataset_builder"
      ],
      "metadata": {
        "id": "5KHSCub50Nua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "from transformers import AutoTokenizer, EncoderDecoderModel\n",
        "from transformers import EncoderDecoderModel, BertTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import create_optimizer, AdamWeightDecay\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "0Ir3roHQ56Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kTWjbv2sz16T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure the encoder and decoder**"
      ],
      "metadata": {
        "id": "SHNtxdu-5-pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
        "\n",
        "BERT is conceptually simple and empirically powerful."
      ],
      "metadata": {
        "id": "YeewFT0UqdHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
        "\n",
        "BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation."
      ],
      "metadata": {
        "id": "nOsxJj7sqoOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPLps1EJ3vds"
      },
      "outputs": [],
      "source": [
        "config_encoder = BertConfig()\n",
        "config_decoder = BertConfig()\n",
        "\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
        "model = EncoderDecoderModel(config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select a tokenizer**<br><br>\n",
        "**BertTokenizer**:\n",
        "BERT uses what is called a WordPiece tokenizer. It works by splitting words either into the full forms (e.g., one word becomes one token) or into word pieces — where one word can be broken into multiple tokens. An example of where this can be useful is where we have multiple forms of words.<br><br>\n",
        "BERT's creators noted a significant decrease in performance when using documents longer than 512 tokens. So, this limit was put to guard against low quality output"
      ],
      "metadata": {
        "id": "-sRSEK4S6Hbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "8a9CaP7o4M3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization"
      ],
      "metadata": {
        "id": "YmX1LC7dub9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization creates a shorter version of a document or an article that captures all the important information.\n",
        "\n",
        "Summarization can be:<br><br\n",
        "\n",
        ">**Extractive**: extract the most relevant information from a document.\n",
        "\n",
        ">**Abstractive**: generate new text that captures the most relevant information."
      ],
      "metadata": {
        "id": "lmWM24irue8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractive Summarization"
      ],
      "metadata": {
        "id": "5EAEh0WLuvdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pretrained model:<br>\n",
        ">**patrickvonplaten/bert2bert_cnn_daily_mail**<br><br>\n",
        "\n",
        "This model is a warm-started BERT2BERT model fine-tuned on the CNN/Dailymail summarization dataset."
      ],
      "metadata": {
        "id": "UQ3WlJvbu01X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a fine-tuned seq2seq model and corresponding tokenizer\n",
        "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\",max_new_tokens=100)"
      ],
      "metadata": {
        "id": "MNDTRg2V68CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter some text to be summarized and tokenize it**"
      ],
      "metadata": {
        "id": "kI8VotbnvA2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's perform inference on a long piece of text\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 \"\n",
        "    \"thousand customers were scheduled to be affected by the shutoffs which were \"\n",
        "    \"expected to last through at least midday tomorrow.\"\n",
        ")\n",
        "input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "hXWnJxYv4Pok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autoregressive models** predict future values based on past values. <br><br>\n",
        "For example: they are widely used in technical analysis to forecast future security prices."
      ],
      "metadata": {
        "id": "nDhi4djQ71bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# autoregressively generate summary (uses greedy decoding by default)\n",
        "generated_ids = model.generate(input_ids)\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "0BISXFkr9Caf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A summary of the ARTICLE_TO_SUMMARIZE**"
      ],
      "metadata": {
        "id": "LpJ4QnntvppI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "Gy9R7kF2riZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model summarized the text<br>\n",
        "Look at the summary the model generated.<br>\n",
        "Would you consider this a good summary?\n"
      ],
      "metadata": {
        "id": "hJJZqr6Q8QV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Another example of extractive summarization**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8zbdRjDj9Yfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's perform inference on a long piece of text\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building,\"\n",
        "    \"and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on \"\n",
        "    \"each side. During its construction, the Eiffel Tower surpassed the Washington Monument to \"\n",
        "    \"become the tallest man-made structure in the world, a title it held for 41 years until the\"\n",
        "    \"Chrysler Building in New York City was finished in 1930. It was the first structure to reach\"\n",
        "    \"a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower\"\n",
        "    \"in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). \"\n",
        "    \"Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France\"\n",
        "    \"after the Millau Viaduct.\"\n",
        ")\n",
        "input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "QvMs_PE88eAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# autoregressively generate summary (uses greedy decoding by default)\n",
        "generated_ids = model.generate(input_ids)\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "VYVsuty17f-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "id": "XnYJF2dC7lO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**<br>\n",
        "What do you think of this summary? <br>\n",
        "Is it correct?"
      ],
      "metadata": {
        "id": "_BOB6eRgsorb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7C-6f7V4_qCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The smaller California state bill subset of the BillSum dataset from the HuggingFace Datasets library"
      ],
      "metadata": {
        "id": "mtLwJkBL_2Zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use a dataset called \"billsum\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "12Y7pEHBwiPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BillSum, summarization of US Congressional and California state bills.<br>\n",
        "\n",
        "There are several features:<br>\n",
        "\n",
        "- text: bill text.<br>\n",
        "- summary: summary of the bills.<br>\n",
        "- title: title of the bills. features for us bills. ca bills does not have.<br>\n",
        "- text_len: number of chars in text.<br>\n",
        "- sum_len: number of chars in summary.<br>\n"
      ],
      "metadata": {
        "id": "vuaZHyirwxe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
      ],
      "metadata": {
        "id": "BY5dYYOG_sRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the dataset into training and test sets**"
      ],
      "metadata": {
        "id": "40uLKx30w-lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billsum = billsum.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "UDgCuuUS_7kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fields that we'll  use:\n",
        "\n",
        "- text: the text of the bill which’ll be the input to the model.<br>\n",
        "- summary: a condensed version of text which’ll be the model target."
      ],
      "metadata": {
        "id": "c5UYDAgFAB0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of the text and summary of one of the bills<br>\n",
        "\n",
        "The summary section comes after the first paragraph break<br>\n",
        "The title comes after the second paragraph break<br>"
      ],
      "metadata": {
        "id": "Yme_-doNxMdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billsum[\"train\"][0]"
      ],
      "metadata": {
        "id": "aXGQ9BIr_-KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize the input**"
      ],
      "metadata": {
        "id": "szFGhpr2yPif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint**\n",
        "When training a PyTorch model with HuggingFace Accelerate, you may want to save and continue a state of training.<bt>\n",
        "\n",
        "To do this requires saving and loading the model, optimizer, RNG generators, and the GradScaler. Inside HuggingFace Accelerate are two convenience functions to achieve this quickly:\n",
        "\n",
        "- Use save_state() for saving everything mentioned above to a folder location\n",
        "- Use load_state() for loading everything stored from an earlier save_state\n",
        "\n",
        "Where and how states can be saved with save_state()\n",
        "\n",
        "For example:<br>\n",
        " >If automatic_checkpoint_naming is enabled each saved checkpoint will be located at:<br>\n",
        "  >>Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}.\n",
        "\n"
      ],
      "metadata": {
        "id": "zsXHEFbfyo7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "QW32PJFWAEfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "pFLSAKRYAG4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "UBgq3ZZOAJcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "N_6su3HzA7q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Including a metric during training can be helpful for evaluating performance"
      ],
      "metadata": {
        "id": "oZXCz8No36t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "kp6sn57gA908"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to calculate the computer metrics"
      ],
      "metadata": {
        "id": "8FRqBLft1SrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "-uukMxYgBR2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
      ],
      "metadata": {
        "id": "O68-3HXwBR47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "zVO7-HvmBW2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the datasets for the model"
      ],
      "metadata": {
        "id": "mlS2H96r1ggo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_billsum[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_billsum[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "q6l7aN_tBayh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile the model**"
      ],
      "metadata": {
        "id": "VIbX8ybc1jl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer)  # No loss argument!"
      ],
      "metadata": {
        "id": "9L8XjHGNBwTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A callback** is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).<br><br>\n",
        "You can use callbacks to: <br>\n",
        "- Write TensorBoard logs after every batch of training to monitor your metrics. <br>\n",
        "- Periodically save your model to disk.\n",
        "- Early Stopping\n",
        "- etc."
      ],
      "metadata": {
        "id": "F5ALdvkM18oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_train_set)"
      ],
      "metadata": {
        "id": "Qsd0iXbfBzIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"summarize: The Inflation Reduction Act lowers prescription drug costs, health care\"\n",
        "       \"costs, and energy costs. It's the most aggressive action on tackling the climate \"\n",
        "       \"crisis in American history, which will lift up American workers and create good-paying,\"\n",
        "       \"union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and \"\n",
        "       \"corporations to pay their fair share. And no one making under $400,000 per year will \"\n",
        "       \"pay a penny more in taxes.\"\n",
        "       )"
      ],
      "metadata": {
        "id": "YOeZwSaRCCnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\")"
      ],
      "metadata": {
        "id": "LmXFpRBrCQ3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_of_text = summarizer(text)"
      ],
      "metadata": {
        "id": "jrNiPo5Bt8NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_of_text"
      ],
      "metadata": {
        "id": "7X5kM9px2jQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"tf\").input_ids"
      ],
      "metadata": {
        "id": "3wZbejl9CUNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\",from_pt=True)\n",
        "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
      ],
      "metadata": {
        "id": "aO0KQe_7CXTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_f_text_again=tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Y8E9yNXNCZd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_f_text_again"
      ],
      "metadata": {
        "id": "pXas1KI13CHK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}