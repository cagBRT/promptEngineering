{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPgEHKZerrCKkwnLPhHAVZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Encoder_Decoder_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/promptEngineering.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "tGP-mAK-mGSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRzbFeCCZBw1"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install sentencepiece==0.1.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MarianMT models**<br>\n",
        "All models are transformer encoder-decoders with 6 layers in each component."
      ],
      "metadata": {
        "id": "wOGXjpya5Z3_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMTXlLtB0Wc-"
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# translate example\n",
        "output_ids = model.generate(input_ids)[0]\n",
        "\n",
        "# decode and print\n",
        "print(tokenizer.decode(output_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-21mDkdwHusE"
      },
      "source": [
        "%%capture\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUSpvzCs0398"
      },
      "source": [
        "embeddings = model.get_input_embeddings()\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# pass input_ids to encoder\n",
        "encoder_hidden_states = model.base_model.encoder(input_ids, return_dict=True).last_hidden_state\n",
        "\n",
        "# change the input slightly and pass to encoder\n",
        "input_ids_perturbed = tokenizer(\"I want to buy a house\", return_tensors=\"pt\").input_ids\n",
        "encoder_hidden_states_perturbed = model.base_model.encoder(input_ids_perturbed, return_dict=True).last_hidden_state\n",
        "\n",
        "# compare shape and encoding of first vector\n",
        "print(f\"Length of input embeddings {embeddings(input_ids).shape[1]}. Length of encoder_hidden_states {encoder_hidden_states.shape[1]}\")\n",
        "\n",
        "# compare values of word embedding of \"I\" for input_ids and perturbed input_ids\n",
        "print(\"Is encoding for `I` equal to its perturbed version?: \", torch.allclose(encoder_hidden_states[0, 0], encoder_hidden_states_perturbed[0, 0], atol=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iZC1-5bb_hQ"
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "embeddings = model.get_input_embeddings()\n",
        "\n",
        "# get encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "decoder_input_ids = tokenizer(\"<pad> Ich will ein\", return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "\n",
        "# pass decoder input_ids and encoded input vectors to decoder\n",
        "decoder_output_vectors = model.base_model.decoder(decoder_input_ids).last_hidden_state\n",
        "\n",
        "# derive embeddings by multiplying decoder outputs with embedding weights\n",
        "lm_logits = torch.nn.functional.linear(decoder_output_vectors, embeddings.weight, bias=model.final_logits_bias)\n",
        "\n",
        "# change the decoder input slightly\n",
        "decoder_input_ids_perturbed = tokenizer(\"<pad> Ich will das\", return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
        "decoder_output_vectors_perturbed = model.base_model.decoder(decoder_input_ids_perturbed).last_hidden_state\n",
        "lm_logits_perturbed = torch.nn.functional.linear(decoder_output_vectors_perturbed, embeddings.weight, bias=model.final_logits_bias)\n",
        "\n",
        "# compare shape and encoding of first vector\n",
        "print(f\"Shape of decoder input vectors {embeddings(decoder_input_ids).shape}. Shape of decoder logits {lm_logits.shape}\")\n",
        "\n",
        "# compare values of word embedding of \"I\" for input_ids and perturbed input_ids\n",
        "print(\"Is encoding for `Ich` equal to its perturbed version?: \", torch.allclose(lm_logits[0, 0], lm_logits_perturbed[0, 0], atol=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, we show exactly what was described earlier. <br>\n",
        "We pass an input \"I want to buy a car\" together with the  BOS  token to the encoder-decoder model and sample from the first logit  ùê•1  (i.e. the first lm_logits line). <br>\n",
        "Hereby, our sampling strategy is simple to greedily choose the next decoder input vector that has the highest probability. <br><br>\n",
        "\n",
        "In an auto-regressive fashion, we then pass the sampled decoder input vector together with the previous inputs to the encoder-decoder model and sample again. We repeat this a third time. <br><br>\n",
        "As a result, the model has generated the words \"Ich Ich\". The first word is spot-on! <br><br>\n",
        "The second word is not that great. We can see here, that a good decoding method is key for a successful sequence generation from a given model distribution."
      ],
      "metadata": {
        "id": "vjOuDMnPlGsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"rnn_seq2seq.png\" , width=800)"
      ],
      "metadata": {
        "id": "PsGE0orFmI5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "CUTqfYhultSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
      ],
      "metadata": {
        "id": "FKLfrLzWlwjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "rGIWzianlyOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "id": "vWCMcPUHmWw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The  BOS  vector** represents the input vector  ùê≤0  fed to the decoder RNN at the very first decoding step."
      ],
      "metadata": {
        "id": "A_CvHah1n9_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EOS  vector** often represents the final input vector ùê±ùëõ to \"cue\" the encoder that the input sequence has ended and also defines the end of the target sequence."
      ],
      "metadata": {
        "id": "tX6BzIITn9Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create BOS token\n",
        "decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "ucdqHj2mmBXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_ids"
      ],
      "metadata": {
        "id": "bKQqRcU0mbFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert decoder_input_ids[0, 0].item() == model.config.decoder_start_token_id, \"`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\""
      ],
      "metadata": {
        "id": "KkVlOde6mD1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Image(\"EncoderDecoder_step_by_step.png\" , width=800)"
      ],
      "metadata": {
        "id": "FjMYeMNuomda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "eCHt8a3MmSDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1\n",
        "# pass input_ids to encoder and to decoder and pass BOS token to decoder to retrieve first logit\n",
        "outputs = model(input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
        "\n",
        "# get encoded sequence\n",
        "encoded_sequence = (outputs.encoder_last_hidden_state,)\n",
        "# get logits\n",
        "lm_logits = outputs.logits\n",
        "\n",
        "# sample last token with highest prob\n",
        "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "\n",
        "# concat\n",
        "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)"
      ],
      "metadata": {
        "id": "-aqImBv5mJ0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "ljYDOHEZmoe4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSSQdRoOZG-w"
      },
      "source": [
        "# STEP 2\n",
        "# reuse encoded_inputs and pass BOS + \"Ich\" to decoder to second logit\n",
        "lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
        "\n",
        "# sample last token with highest prob again\n",
        "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "\n",
        "# concat again\n",
        "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3"
      ],
      "metadata": {
        "id": "3JjE_M52msp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3\n",
        "lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
        "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
        "\n",
        "# let's see what we have generated so far!\n",
        "print(f\"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n",
        "\n",
        "# This can be written in a loop as well.\n",
        "#Ich will ein Auto kaufen --> full translation"
      ],
      "metadata": {
        "id": "rFQoo2IimM18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}