{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPE9qBghkMwH0lwZS7UDU4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/promptEngineering/blob/main/Embedding_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Text Classification Model"
      ],
      "metadata": {
        "id": "S8V1Xz7BlHN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the libraries**"
      ],
      "metadata": {
        "id": "N1kA1YJflSwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding"
      ],
      "metadata": {
        "id": "8LND7lp-VZEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create text statements that will be classified as positive or negative**"
      ],
      "metadata": {
        "id": "7aNqc2xxlXhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define documents\n",
        "docs = ['Well done!',\n",
        " 'Good work',\n",
        " 'Great effort',\n",
        " 'nice work',\n",
        " 'Excellent!',\n",
        " 'Weak',\n",
        " 'Poor effort!',\n",
        " 'not good',\n",
        " 'poor work',\n",
        " 'Could have done better.']"
      ],
      "metadata": {
        "id": "5igSQVtkVbc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the labels** <br>\n",
        ">1=positive statement<br>\n",
        "0=negative statement"
      ],
      "metadata": {
        "id": "fGPEPzGAlj4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "2nfwZdkwVd33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the vocabulary size and encode the text statements**"
      ],
      "metadata": {
        "id": "CEbu_tTWlug1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "for i in range(len(docs)):\n",
        "  print(docs[i], encoded_docs[i])"
      ],
      "metadata": {
        "id": "mRPHvvnpVgg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the length of the input text to four<br>\n",
        "Pad any short sequences"
      ],
      "metadata": {
        "id": "Yrou_etGnMng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "for i in range(len(docs)):\n",
        "  print(docs[i], padded_docs[i])"
      ],
      "metadata": {
        "id": "TiHvDITcVlKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model**<br>\n"
      ],
      "metadata": {
        "id": "0YK20ZGhnfnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Keras Embedding Layer<br>\n",
        "tf.keras.layers.Embedding(<br>\n",
        "  >  input_dim,<br>\n",
        "    output_dim,<br>\n",
        "    embeddings_initializer=\"uniform\",<br>\n",
        "    embeddings_regularizer=None,<br>\n",
        "    activity_regularizer=None,<br>\n",
        "    embeddings_constraint=None,<br>\n",
        "    mask_zero=False,<br>\n",
        "    input_length=None,<br>\n",
        "    sparse=False,<br>\n",
        ")"
      ],
      "metadata": {
        "id": "uEeKk-uTn7K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**input_dim:** Integer. Size of the vocabulary, i.e. maximum integer index + 1.<br>\n",
        "**output_dim**: Integer. Dimension of the dense embedding.<br>\n",
        "**embeddings_initializer**: Initializer for the embeddings matrix (see keras.initializers).<br>\n",
        "embeddings_regularizer: Regularizer function applied to the embeddings matrix (see keras.regularizers).<br>\n",
        "embeddings_constraint: Constraint function applied to the embeddings matrix (see keras.constraints).<br>\n",
        "**mask_zero**: Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True, then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).<br>\n",
        "**input_length**: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).<br>\n",
        "**sparse**: If True, calling this layer returns a tf.SparseTensor. If False, the layer returns a dense tf.Tensor. For an entry with no features in a sparse tensor (entry with value 0), the embedding vector of index 0 is returned by default.<br>"
      ],
      "metadata": {
        "id": "OUhvV-r1oQK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "jUIpsWuuV6SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Evaluate the model**"
      ],
      "metadata": {
        "id": "FGkg2lhypBpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)"
      ],
      "metadata": {
        "id": "36tZ9lVPWAW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Mqq8qOC6CW"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment<br>\n",
        "1. Add more docs to the dataset. Does the accuracy change?\n",
        "2. Add an additional label to the dataset - pos, neg, neutral. Does the accuracy change?"
      ],
      "metadata": {
        "id": "1jzdFsSnpTPS"
      }
    }
  ]
}